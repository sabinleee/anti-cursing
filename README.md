# anti-cursing

**"anti-cursing"** is a python package that detects and switches negative or any kind of cursing word from sentences or comments whateverğŸ¤¬

You just install the package the way you install any other package and then you can use it in your code.

The whole thing is gonna be updated soon.

So this is __**the very first idea**__

But you can find my package in pypi(https://pypi.org/project/anti-cursing/0.0.1/)

**ğŸ™ğŸ»Plz bare with the program to install model's weight and bias from huggingface at the first time you use the package.**

<img width="1134" alt="image" src="https://user-images.githubusercontent.com/50198431/203723736-3aeb84a1-6418-4190-b967-2888e14b14fd.png">

<hr>

# Concept
There are often situations where you have to code something, detect a forbidden word, and change it to another word.
Hardcoding all parts is very inconvenient, and in the Python ecosystem, there are many packages to address.
One of them is **"anti-cursing"**.

The package, which operates exclusively for **Korean**, does not simply change the banned word by setting it up, but detects and replaces the banned word by learning a deep learning model.

Therefore, it is easy to cope with new malicious words as long as they are learned.
For this purpose, semi-supervied learning through pseudo labeling is used.

Additionally, instead of changing malicious words to special characters such as --- or ***, you can convert them into emojis to make them more natural.

# Contents

- [Installation](#installation)
- [Usage](#usage)
- [Model comparison](#model-comparison)
- [Dataset](#dataset)
- [Used API](#used-api)
- [License](#license)
- [Working Example](#working-example)
- [References](#references)
- [Project Status](#project-status)
- [Future Work](#future-work)


# Installation

You can install the package using pip:

```bash
pip install anti-cursing
```

**it doesn't work yet, but it will soon!!ğŸ‘¨ğŸ»â€ğŸ’»**

# Usage

```python
from anti_cursing.utils import antiCursing

antiCursing.anti_cur("ë‚˜ëŠ” ë„ˆê°€ ì¢‹ì§€ë§Œ, ë„ˆëŠ” ë„ˆë¬´ ê°œìƒˆë¼ì•¼")
```

```bash
ë‚˜ëŠ” ë„ˆê°€ ì¢‹ì§€ë§Œ, ë„ˆëŠ” ë„ˆë¬´ ğŸ‘¼ğŸ»ì•¼
```

# Model-comparison
| Classification | KcElectra | KoBERT | RoBERTa-base | RoBERTa-large |
| --- | --- | --- | --- | --- |
| Validation Accuracy | 0.88680 | 0.85721 | 0.83421 | 0.86994 |
| Validation Loss | 1.00431 | 1.23237 | 1.30012 | 1.16179 |
| Training Loss | 0.09908 | 0.03761 | 0.0039 | 0.06255 |
| Epoch | 10 | 40 | 20 | 20 |
| Batch-size | 8 | 32 | 16 | 32 |
| transformers | beomi/KcELECTRA-base | skt/kobert-base-v1 | xlm-roberta-base | klue/roberta-large |

# Dataset
* ### Smilegate-AI 
  * https://github.com/smilegate-ai/korean_unsmile_dataset
  * Korean Sentiment Analysis
  * [paper](#korean-unsmile-dataset)
* ### Naver portal news articles crawling
  * https://news.naver.com
  * Non-labeled Data for Test Dataset
* ### ğŸ˜€ Emoji unicode crawling for encoding
  * https://unicode.org/emoji/charts/full-emoji-list.html

# Used-api
### Google translator
* https://cloud.google.com/translate/docs (API DOCS)

# License

This repository is licensed under the MIT license. See LICENSE for details.

Click here to see the License information --> [License](LICENSE)

# Working-example
---- some video is gonna be placed here ----

# References

### Sentiment Analysis Based on Deep Learning : A Comparative Study
  * Nhan Cach Dang, Maria N. Moreno-Garcia, Fernando De la Prieta. 2006. Sentiment Analysis Based on Deep Learning : A Comparative Study. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 1â€“8, Prague, Czech Republic. Association for Computational Linguistics.
### Attention is all you need
  * Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000â€“6010.
### BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding
  * Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:         Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 4171â€“4186.

### Electra : Pre-training Text Encoders as Discriminators Rather Than Generators
  * Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. 2019. Electra: Pre-training text encoders as discriminators rather than generators. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 4171â€“4186.

### BIDAF : Bidirectional Attention Flow for Machine Comprehension
  * Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi. 2016. Bidirectional Attention Flow for Machine Comprehension. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2129â€“2139.

### Effect of Negation in Sentences on Sentiment Analysis and Polarity Detection
  * Partha Mukherjeea, Saptarshi Ghoshb, and Saptarshi Ghoshc. 2018. Effect of Negation in Sentences on Sentiment Analysis and Polarity Detection. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2129â€“2139.

### KOAS : Korean Text Offensiveness Analysis System
  * Seonghwan Kim, Seongwon Lee, and Seungwon Do. 2019. KOAS: Korean Text Offensiveness Analysis System. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1â€“11.

### Korean Unsmile Dataset
  * Seonghwan Kim, Seongwon Lee, and Seungwon Do. 2019. Korean Unsmile Dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1â€“11.
# Project-status

![80%](https://geps.dev/progress/80)

# Future-work
update soon plz bare with me ğŸ™ğŸ»

<hr>

# KOREAN FROM HERE / ì—¬ê¸°ë¶€í„´ í•œêµ­ì–´ ì„¤ëª…ì…ë‹ˆë‹¤.
# anti-cursing

**"anti-cursing"**ì€ ë¬¸ì¥ì´ë‚˜ ëŒ“ê¸€ì—ì„œ ë¶€ì •ì ì´ê±°ë‚˜ ëª¨ë“  ì¢…ë¥˜ì˜ ìš•ì„¤ì„ ê°ì§€í•˜ê³  ì „í™˜í•˜ëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤ğŸ¤¬

ë‹¤ë¥¸ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ëŠ” ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œ ë‹¤ìŒ ì½”ë“œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ì§ ì•„ì´ë””ì–´ êµ¬ìƒ ë‹¨ê³„ì´ê¸° ë•Œë¬¸ì— **ì•„ë¬´ê²ƒë„ ì‘ë™í•˜ì§€ ì•Šì§€ë§Œ** ê³§ ì‘ë™í•˜ë„ë¡ ì—…ë°ì´íŠ¸í•  ì˜ˆì •ì…ë‹ˆë‹¤.

Pypi(https://pypi.org/project/anti-cursing/0.0.1/)ì— íŒ¨í‚¤ì§€ë¥´ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤. í™•ì¸í•˜ì‹œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ğŸ™ğŸ»íŒ¨í‚¤ì§€ë¥¼ ì²˜ìŒ ì„¤ì¹˜í•˜ì‹œê³  ì‚¬ìš©í•˜ì‹¤ ë•Œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ huggingfaceì—ì„œ parsingì„ ì‹œë„í•©ë‹ˆë‹¤. ì²˜ìŒì—ë§Œ í•´ë‹¹ ì‘ì—…ì´ í•„ìš”í•˜ë‹ˆ ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦¼ê³¼ ìš©ëŸ‰ì„ ì°¨ì§€í•¨ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”**

<img width="1134" alt="image" src="https://user-images.githubusercontent.com/50198431/203723736-3aeb84a1-6418-4190-b967-2888e14b14fd.png">


<hr>

# Concept

ë¬´ì–¸ê°€ ì½”ë”©ì„ í•˜ë©°, ê¸ˆì§€ ë‹¨ì–´ë¥¼ ê°ì§€í•˜ê³  ê·¸ê²ƒì„ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ë°”ê¿”ì•¼í•  ìƒí™©ì´ ì¢…ì¢… ìƒê¹ë‹ˆë‹¤.
ëª¨ë“  ë¶€ë¶„ì„ í•˜ë“œì½”ë”©í•˜ëŠ” ê²ƒì´ ë§¤ìš° ë¶ˆí¸í•˜ë©°, íŒŒì´ì¬ ìƒíƒœê³„ì—ì„œëŠ” ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë§ì€ íŒ¨í‚¤ì§€ê°€ ìˆìŠµë‹ˆë‹¤.
ê·¸ ì¤‘ í•˜ë‚˜ê°€ **"anti-cursing"**ì…ë‹ˆë‹¤.

í•œêµ­ì–´ ì „ìš©ìœ¼ë¡œ ë™ì‘í•˜ëŠ” í•´ë‹¹ íŒ¨í‚¤ì§€ëŠ” ë‹¨ìˆœíˆ ê¸ˆì§€ ë‹¨ì–´ë¥¼ ê¸°ì¡´ì— ì„¤ì •í•˜ì—¬ ë°”ê¾¸ëŠ” ê²ƒì´ ì•„ë‹Œ, ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ê¸ˆì§€ ë‹¨ì–´ë¥¼ ê°ì§€í•˜ê³  ë°”ê¿‰ë‹ˆë‹¤.
ë”°ë¼ì„œ ìƒˆë¡­ê²Œ ìƒê¸°ëŠ” ì•…ì„± ë‹¨ì–´ì— ëŒ€í•´ì„œë„ í•™ìŠµë§Œ ì´ë£¨ì–´ì§„ë‹¤ë©´ ì‰½ê²Œ ëŒ€ì²˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë¥¼ ìœ„í•´ pseudo labelingì„ í†µí•œ semi-supervied learningì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 

ì¶”ê°€ë¡œ ì•…ì„±ë‹¨ì–´ë¥¼ ---ë‚˜ ***ê°™ì€ íŠ¹ìˆ˜ë¬¸ìë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ì´ëª¨ì§€ë¡œ ë³€í™˜í•˜ì—¬ ë”ìš± ìì—°ìŠ¤ëŸ½ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ëª©ì°¨

- [ì„¤ì¹˜](#ì„¤ì¹˜)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ](#ëª¨ë¸-ì„±ëŠ¥-ë¹„êµ)
- [ë°ì´í„°ì…‹](#ë°ì´í„°ì…‹)
- [ì‚¬ìš© API](#ì‚¬ìš©-api)
- [License](#license)
- [ì‘ë™ ì˜ˆì‹œ](#ì‘ë™-ì˜ˆì‹œ)
- [ì°¸ê³ ë¬¸í—Œ](#ì°¸ê³ ë¬¸í—Œ)
- [ì§„í–‰ìƒí™©](#ì§„í–‰ìƒí™©)
- [ë°œì „](#ë°œì „)


# ì„¤ì¹˜

pipë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```bash
pip install anti-cursing
```

**ì•„ì§ ì•„ë¬´ê²ƒë„ ì‘ë™í•˜ì§€ ì•Šì§€ë§Œ, ê³§ ì‘ë™í•˜ë„ë¡ ì—…ë°ì´íŠ¸í•  ì˜ˆì •ì…ë‹ˆë‹¤ğŸ‘¨ğŸ»â€ğŸ’».**

# ì‚¬ìš©ë²•

```python
from anti_cursing.utils import antiCursing

antiCursing.anti_cur("ë‚˜ëŠ” ë„ˆê°€ ì¢‹ì§€ë§Œ, ë„ˆëŠ” ë„ˆë¬´ ê°œìƒˆë¼ì•¼")
```

```bash
ë‚˜ëŠ” ë„ˆê°€ ì¢‹ì§€ë§Œ, ë„ˆëŠ” ë„ˆë¬´ ğŸ‘¼ğŸ»ì•¼
```

# ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
| Classification | KcElectra | KoBERT | RoBERTa-base | RoBERTa-large |
| --- | --- | --- | --- | --- |
| Validation Accuracy | 0.88680 | 0.85721 | 0.83421 | 0.86994 |
| Validation Loss | 1.00431 | 1.23237 | 1.30012 | 1.16179 |
| Training Loss | 0.09908 | 0.03761 | 0.0039 | 0.06255 |
| Epoch | 10 | 40 | 20 | 20 |
| Batch-size | 8 | 32 | 16 | 32 |
| transformers | beomi/KcELECTRA-base | skt/kobert-base-v1 | xlm-roberta-base | klue/roberta-large |


# ë°ì´í„°ì…‹
* ### Smilegate-AI 
  * https://github.com/smilegate-ai/korean_unsmile_dataset
  * í•œêµ­ì–´ ê°ì •ë¶„ë¥˜ ë°ì´í„°ì…‹
  * [paper](#korean-unsmile-dataset)
* ### ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§
  * https://news.naver.com
  * í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹
* ### ğŸ˜€ ì´ëª¨ì§€ ìœ ë‹ˆì½”ë“œ ë°ì´í„°ì…‹
  * https://unicode.org/emoji/charts/full-emoji-list.html

# ì‚¬ìš© API
### Google translator
* https://cloud.google.com/translate/docs (API ë¬¸ì„œ)

# License

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ LICENSE íŒŒì¼ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

ë¼ì´ì„¼ìŠ¤ ì •ë³´ --> [License](LICENSE)

# ì‘ë™ ì˜ˆì‹œ
---- ì‘ë™ ì˜ˆì‹œê°€ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤ ----

# ì°¸ê³ ë¬¸í—Œ

### Sentiment Analysis Based on Deep Learning : A Comparative Study
  * Nhan Cach Dang, Maria N. Moreno-Garcia, Fernando De la Prieta. 2006. Sentiment Analysis Based on Deep Learning : A Comparative Study. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 1â€“8, Prague, Czech Republic. Association for Computational Linguistics.
### Attention is all you need
  * Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000â€“6010.
### BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding
  * Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:         Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 4171â€“4186.

### Electra : Pre-training Text Encoders as Discriminators Rather Than Generators
  * Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. 2019. Electra: Pre-training text encoders as discriminators rather than generators. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 4171â€“4186.

### BIDAF : Bidirectional Attention Flow for Machine Comprehension
  * Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi. 2016. Bidirectional Attention Flow for Machine Comprehension. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2129â€“2139.

### Effect of Negation in Sentences on Sentiment Analysis and Polarity Detection
  * Partha Mukherjeea, Saptarshi Ghoshb, and Saptarshi Ghoshc. 2018. Effect of Negation in Sentences on Sentiment Analysis and Polarity Detection. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2129â€“2139.

### KOAS : Korean Text Offensiveness Analysis System
  * Seonghwan Kim, Seongwon Lee, and Seungwon Do. 2019. KOAS: Korean Text Offensiveness Analysis System. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1â€“11.

### Korean Unsmile Dataset
  * Seonghwan Kim, Seongwon Lee, and Seungwon Do. 2019. Korean Unsmile Dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1â€“11.
# ì§„í–‰ìƒí™©

![80%](https://geps.dev/progress/80)

# ë°œì „
ì•ìœ¼ë¡œ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”ğŸ™ğŸ»
